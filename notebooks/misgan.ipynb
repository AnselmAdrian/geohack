{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MisGAN: Learning from Incomplete Data with GANs\n",
    "\n",
    "[Generative adversarial networks](https://arxiv.org/abs/1406.2661) (GANs)\n",
    "provide a powerful modeling framework for learning complex\n",
    "high-dimensional distributions.\n",
    "Training GANs normally requires access to a large collection of\n",
    "fully-observed data. However, it is not always possible to obtain a\n",
    "large amount of fully-observed data. Missing data is well-known to be\n",
    "prevalent in many real-world application domains where different data\n",
    "cases might have different missing entries. This arbitrary missingness\n",
    "poses a significant challenge to many existing machine learning models.\n",
    "\n",
    "In this notebook, we present a quick introduction to\n",
    "[MisGAN](https://openreview.net/forum?id=S1lDV3RcKm&noteId=S1lDV3RcKm),\n",
    "a GAN-based framework for learning from incomplete data.\n",
    "We demonstrate how to implement MisGAN in [PyTorch](https://pytorch.org/)\n",
    "and run it on a modified MNIST dataset where the images are partially-observed.\n",
    "\n",
    "## Missing data\n",
    "\n",
    "To understand the design of MisGAN, we first talk about how to formally\n",
    "model missing data.\n",
    "The generative process for incompletely observed\n",
    "data can be described below where $\\mathbf{x}\\in\\mathbb{R}^n$\n",
    "is a complete data vector and $\\mathbf{m}\\in\\{0,1\\}^n$\n",
    "is a binary mask that determines which entries in $\\mathbf{x}$ to reveal:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "  \\mathbf{x}&\\sim p_\\theta(\\mathbf{x}), \\\\\n",
    "  \\quad\\mathbf{m}&\\sim p_\\phi(\\mathbf{m}|\\mathbf{x}).\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We can represent an incomplete data instance as a pair of\n",
    "a partially-observed data vector $\\mathbf{x}\\in\\mathbb{R}^n$ and\n",
    "a corresponding mask $\\mathbf{m}\\in\\{0,1\\}^n$\n",
    "that indicates which entries in $\\mathbf{x}$ are observed:\n",
    "$x_d$ is observed if $m_d=1$ otherwise $x_d$ is missing and might\n",
    "contain an arbitrary value that we should ignore.\n",
    "With this representation, an incomplete dataset is in the form of\n",
    "$\\mathcal{D}=\\{(\\mathbf{x}_i,\\mathbf{m}_i)\\}_{i=1,\\dots,N}$\n",
    "where both $\\mathbf{x}_i$ and $\\mathbf{m}_i$ are fixed-length vectors.\n",
    "\n",
    "Before introducing MisGAN, we first create an incomplete dataset for the rest\n",
    "of the experiments.\n",
    "We start with configuring the notebook and importing required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade torch torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.autograd import grad\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from matplotlib.patches import Rectangle\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda' if use_cuda else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incomplete MNIST dataset\n",
    "\n",
    "In this notebook, all the experiments will be run on the\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset,\n",
    "which contains 60000 handwritten digits images of size 28x28.\n",
    "For the missing data distribution,\n",
    "we choose the \"square observation\" pattern: all pixels are missing\n",
    "except for a square occurring at a random location on the image.\n",
    "For simplicity, we assume that there is no dependency between the mask and\n",
    "the content of the image.\n",
    "This is also known as missing completely at random (MCAR).\n",
    "\n",
    "`BlockMaskedMNIST` creates an incomplete MNIST dataset by turning each image\n",
    "in MNIST into a pair of a partially-observed image (with type `FloatTensor`)\n",
    "and a mask (with type `CharTensor`), which both have size (1, 28, 28).\n",
    "Note that the range of pixel values of each image is rescaled to \\[0,1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockMaskedMNIST(Dataset):\n",
    "    def __init__(self, block_len, data_dir='src/mnist-data', random_seed=0):\n",
    "        self.block_len = block_len\n",
    "        self.rnd = np.random.RandomState(random_seed)\n",
    "        data = datasets.MNIST(data_dir, train=True, download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "        self.data_size = len(data)\n",
    "        self.generate_incomplete_data(data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return index so we can retrieve the mask location from self.mask_loc\n",
    "        return self.image[index], self.mask[index], index\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def generate_incomplete_data(self, data):\n",
    "        n_masks = self.data_size\n",
    "        self.image = [None] * n_masks\n",
    "        self.mask = [None] * n_masks\n",
    "        self.mask_loc = [None] * n_masks\n",
    "        for i in range(n_masks):\n",
    "            d0 = self.rnd.randint(0, 28 - self.block_len + 1)\n",
    "            d1 = self.rnd.randint(0, 28 - self.block_len + 1)\n",
    "            mask = torch.zeros((28, 28), dtype=torch.uint8)\n",
    "            mask[d0:(d0 + self.block_len), d1:(d1 + self.block_len)] = 1\n",
    "            self.mask[i] = mask.unsqueeze(0)   # add an axis for channel\n",
    "            self.mask_loc[i] = d0, d1, self.block_len, self.block_len\n",
    "            # Mask out missing pixels by zero\n",
    "            self.image[i] = data[i][0] * mask.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a modified MNIST dataset using `BlockMaskedMNIST` with a \n",
    "random 12x12 observed block on each image, which accounts for \n",
    "81.6% missing rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = BlockMaskedMNIST(block_len=12)\n",
    "batch_size = 64\n",
    "data_loader = DataLoader(data, batch_size=batch_size, shuffle=True,\n",
    "                         drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we implement `plot_grid()` for plotting input images on a grid\n",
    "of `nrow` rows and `ncol` columns.\n",
    "An optional argument `bbox` can be provided as a list of (x, y, width, height)\n",
    "to draw a red rectangular frame with that coordinate on each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grid(ax, image, bbox=None, gap=1, gap_value=1, nrow=4, ncol=8,\n",
    "              title=None):\n",
    "    image = image.cpu().numpy().squeeze(1)\n",
    "    LEN = 28\n",
    "    grid = np.empty((nrow * (LEN + gap) - gap, ncol * (LEN + gap) - gap))\n",
    "    grid.fill(gap_value)\n",
    "\n",
    "    for i, x in enumerate(image):\n",
    "        if i >= nrow * ncol:\n",
    "            break\n",
    "        p0 = (i // ncol) * (LEN + gap)\n",
    "        p1 = (i % ncol) * (LEN + gap)\n",
    "        grid[p0:(p0 + LEN), p1:(p1 + LEN)] = x\n",
    "\n",
    "    ax.set_axis_off()\n",
    "    ax.imshow(grid, cmap='binary_r', interpolation='none', aspect='equal')\n",
    "\n",
    "    if bbox:\n",
    "        nplot = min(len(image), nrow * ncol)\n",
    "        for i in range(nplot):\n",
    "            d0, d1, d0_len, d1_len = bbox[i]\n",
    "            p0 = (i // ncol) * (LEN + gap)\n",
    "            p1 = (i % ncol) * (LEN + gap)\n",
    "            offset = np.array([p1 + d1, p0 + d0]) - .5\n",
    "            ax.add_patch(Rectangle(\n",
    "                offset, d1_len, d0_len, lw=1.5, edgecolor='red', fill=False))\n",
    "            \n",
    "    if title:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking operator\n",
    "\n",
    "Here we implement the masking operator\n",
    "$f_\\tau(\\mathbf{x}, \\mathbf{m}) = \\mathbf{x} \\odot \\mathbf{m} + \\tau\\bar{\\mathbf{m}}$.\n",
    "\n",
    "As we mentioned before, an incomplete data instance can be represented as\n",
    "a pair of fixed-length vectors $(\\mathbf{x}, \\mathbf{m})$.\n",
    "The masking operator transforms an incomplete data instance into a vector of\n",
    "the same size with all missing entries in $\\mathbf{x}$ replaced by a constant\n",
    "value $\\tau$.\n",
    "This plays an important role in MisGAN that we will describe later.\n",
    "Before that, we will use it to visualize the incomplete MNIST dataset\n",
    "we just prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_data(data, mask, tau=0):\n",
    "    return mask * data + (1 - mask) * tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of incomplete MNIST images\n",
    "\n",
    "We plot a random subset of images from the incomplete MNIST dataset below.\n",
    "Gray pixels represent the missing entries in each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples, mask_samples, _ = next(iter(data_loader))\n",
    "fig, ax = plt.subplots(figsize=(12, 3))\n",
    "plot_grid(ax, mask_data(data_samples, mask_samples.float(), .5),\n",
    "          nrow=4, ncol=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MisGAN\n",
    "\n",
    "MisGAN is a GAN-based framework for learning distributions in the presence\n",
    "of incomplete observations. The overall structure is illustrated below:\n",
    "\n",
    "<img src=\"img/misgan.png\" width=\"500\" style=\"display: block; margin: 2em auto\" />\n",
    "\n",
    "MisGAN consists of a data generator $G_x$ that generates complete data.\n",
    "In addition, it also has a mask generator $G_m$ to explicitly model the\n",
    "missing data process. \n",
    "Note that the input $(\\mathbf{x}, \\mathbf{m})$ is the incomplete data that\n",
    "follows the representation mentioned earlier.\n",
    "\n",
    "MisGAN mimics the generation of incomplete data (under the MCAR assumption)\n",
    "by treating the generated complete data $\\tilde{\\mathbf{x}}$ together with\n",
    "the generated mask $\\tilde{\\mathbf{m}}$ as an incomplete data instance,\n",
    "with the mask specifies which entries in $\\tilde{\\mathbf{x}}$ are considered\n",
    "missing.\n",
    "\n",
    "We then complete both the real incomplete data $(\\mathbf{x}, \\mathbf{m})$\n",
    "and the generated ones $(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{m}})$\n",
    "using the same masking operator $f_{\\tau}$.\n",
    "We train the data generator $G_x$ by making the masked generated data\n",
    "$f_{\\tau}(\\tilde{\\mathbf{x}}, \\tilde{\\mathbf{m}})$ indistinguishable from\n",
    "the masked real incomplete data $f_{\\tau}(\\mathbf{x}, \\mathbf{m})$ using\n",
    "a data discriminator $D_x$.\n",
    "\n",
    "On the other hand, since both generated masks $\\tilde{\\mathbf{m}}$ and\n",
    "real masks $\\mathbf{m}$ are fully-observed, we can train the mask generator\n",
    "$G_m$ with a mask discriminator $D_m$ as in a standard GAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n",
    "For each generator, we use a linear layer followed by three deconvolution\n",
    "layers with ReLUs in between.\n",
    "\n",
    "`ConvDataGenerator` implements the data generator $G_x$.\n",
    "Since the pixel values are in \\[0, 1\\], we apply the sigmoid activation to\n",
    "the real-valued output at the end.\n",
    "\n",
    "`ConvMaskGenerator` implements the mask generator $G_m$.\n",
    "Note that the masks are binary-valued.\n",
    "Since discrete data generating processes have zero gradient almost everywhere,\n",
    "to carry out gradient-based training for GANs, we relax the output\n",
    "of the mask generator $G_m$ from $\\{0,1\\}^n$ to $[0, 1]^n$.\n",
    "We use the sigmoid activated output $\\sigma(z / \\lambda)$\n",
    "with a low temperature $\\lambda = 0.66$ to encourage saturation\n",
    "and make the output closer to zero or one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must sub-class ConvGenerator to provide transform()\n",
    "class ConvGenerator(nn.Module):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DIM = 64\n",
    "        self.latent_size = latent_size\n",
    "\n",
    "        self.preprocess = nn.Sequential(\n",
    "            nn.Linear(latent_size, 4 * 4 * 4 * self.DIM),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(4 * self.DIM, 2 * self.DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.block2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(2 * self.DIM, self.DIM, 5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.deconv_out = nn.ConvTranspose2d(self.DIM, 1, 8, stride=2)\n",
    "\n",
    "    def forward(self, input):\n",
    "        net = self.preprocess(input)\n",
    "        net = net.view(-1, 4 * self.DIM, 4, 4)\n",
    "        net = self.block1(net)\n",
    "        net = net[:, :, :7, :7]\n",
    "        net = self.block2(net)\n",
    "        net = self.deconv_out(net)\n",
    "        return self.transform(net).view(-1, 1, 28, 28)\n",
    "\n",
    "\n",
    "class ConvDataGenerator(ConvGenerator):\n",
    "    def __init__(self, latent_size=128):\n",
    "        super().__init__(latent_size=latent_size)\n",
    "        self.transform = lambda x: torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class ConvMaskGenerator(ConvGenerator):\n",
    "    def __init__(self, latent_size=128, temperature=.66):\n",
    "        super().__init__(latent_size=latent_size)\n",
    "        self.transform = lambda x: torch.sigmoid(x / temperature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "We implement the discriminator (or referred to as the critic in\n",
    "Wasserstein GANs) in `ConvCritic` with three convolutional layers followed by\n",
    "a linear layer for both $D_x$ and $D_m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.DIM = 64\n",
    "        main = nn.Sequential(\n",
    "            nn.Conv2d(1, self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.DIM, 2 * self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(2 * self.DIM, 4 * self.DIM, 5, stride=2, padding=2),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.main = main\n",
    "        self.output = nn.Linear(4 * 4 * 4 * self.DIM, 1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        input = input.view(-1, 1, 28, 28)\n",
    "        net = self.main(input)\n",
    "        net = net.view(-1, 4 * 4 * 4 * self.DIM)\n",
    "        net = self.output(net)\n",
    "        return net.view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Wasserstein GAN with gradient penalty\n",
    "\n",
    "MisGAN is compatible with many GAN variations, and here we use\n",
    "[Wasserstein GAN](https://arxiv.org/abs/1701.07875) to train MisGAN.\n",
    "`CriticUpdater` computes the loss of the discriminator and updates \n",
    "its parameters accordingly.\n",
    "We follow the [WGAN-GP](https://arxiv.org/abs/1704.00028) procedure to\n",
    "train discriminators with the gradient penalty.\n",
    "\n",
    "Specifically,\n",
    "given the data distribution $p_r$ and the model distribution $p_g$,\n",
    "the loss for the discriminator $D$ is given by\n",
    "\n",
    "$$\n",
    "\\mathcal{L} =\n",
    "\\mathbb{E}_{\\tilde{\\mathbf{x}}\\sim p_g}\\big[D(\\tilde{\\mathbf{x}})\\big] -\n",
    "\\mathbb{E}_{\\mathbf{x}\\sim p_r}\\big[D(\\mathbf{x})\\big] +\n",
    "\\lambda \\mathbb{E}_{\\mathbf{y}}\n",
    "\\big[\\left(\\|\\nabla_{\\mathbf{y}} D(\\mathbf{y})\\|_2 - 1\\right)^2\\big]\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ is sampled according to\n",
    "$\\mathbf{y}=\\xi\\mathbf{x} + (1-\\xi)\\tilde{\\mathbf{x}}$ with\n",
    "$\\tilde{\\mathbf{x}}\\sim p_g$, $\\mathbf{x}\\sim p_r$,\n",
    "and $\\xi\\sim\\operatorname{uniform}(0, 1)$.\n",
    "The gradient penalty term is for enforcing the (soft) 1-Lipschtiz\n",
    "constraint required by the Wasserstein GAN.\n",
    "\n",
    "Samples drawn from $p_r$ and $p_g$ are provided as `real` and `fake`\n",
    "respectively when calling `CriticUpdater`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticUpdater:\n",
    "    def __init__(self, critic, critic_optimizer, batch_size=64, gp_lambda=10):\n",
    "        self.critic = critic\n",
    "        self.critic_optimizer = critic_optimizer\n",
    "        self.gp_lambda = gp_lambda\n",
    "        # Interpolation coefficient\n",
    "        self.eps = torch.empty(batch_size, 1, 1, 1, device=device)\n",
    "        # For computing the gradient penalty\n",
    "        self.ones = torch.ones(batch_size).to(device)\n",
    "\n",
    "    def __call__(self, real, fake):\n",
    "        real = real.detach()\n",
    "        fake = fake.detach()\n",
    "        self.critic.zero_grad()\n",
    "        self.eps.uniform_(0, 1)\n",
    "        interp = (self.eps * real + (1 - self.eps) * fake).requires_grad_()\n",
    "        grad_d = grad(self.critic(interp), interp, grad_outputs=self.ones,\n",
    "                      create_graph=True)[0]\n",
    "        grad_d = grad_d.view(real.shape[0], -1)\n",
    "        grad_penalty = ((grad_d.norm(dim=1) - 1)**2).mean() * self.gp_lambda\n",
    "        w_dist = self.critic(fake).mean() - self.critic(real).mean()\n",
    "        loss = w_dist + grad_penalty\n",
    "        loss.backward()\n",
    "        self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate all the building blocks for MisGAN: the data/mask\n",
    "generators and their corresponding discriminators.\n",
    "We use the [Adam optimizer](https://arxiv.org/abs/1412.6980) to train them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nz = 128   # dimensionality of the latent code\n",
    "n_critic = 5\n",
    "alpha = .2\n",
    "\n",
    "data_gen = ConvDataGenerator().to(device)\n",
    "mask_gen = ConvMaskGenerator().to(device)\n",
    "\n",
    "data_critic = ConvCritic().to(device)\n",
    "mask_critic = ConvCritic().to(device)\n",
    "\n",
    "data_noise = torch.empty(batch_size, nz, device=device)\n",
    "mask_noise = torch.empty(batch_size, nz, device=device)\n",
    "\n",
    "lrate = 1e-4\n",
    "data_gen_optimizer = optim.Adam(\n",
    "    data_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
    "mask_gen_optimizer = optim.Adam(\n",
    "    mask_gen.parameters(), lr=lrate, betas=(.5, .9))\n",
    "\n",
    "data_critic_optimizer = optim.Adam(\n",
    "    data_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
    "mask_critic_optimizer = optim.Adam(\n",
    "    mask_critic.parameters(), lr=lrate, betas=(.5, .9))\n",
    "\n",
    "update_data_critic = CriticUpdater(\n",
    "    data_critic, data_critic_optimizer, batch_size)\n",
    "update_mask_critic = CriticUpdater(\n",
    "    mask_critic, mask_critic_optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MisGAN\n",
    "\n",
    "To describe the training procedure of MisGAN,\n",
    "we first define the following loss functions,\n",
    "one for the masks and the other for the data:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "  \\mathcal{L}_m(D_m, G_m) &=\n",
    "  \\mathbb{E}_{(\\mathbf{x},\\mathbf{m})\\sim p_\\mathcal{D}}\\left[D_m(\\mathbf{m})\\right] -\n",
    "  \\mathbb{E}_{\\boldsymbol{\\varepsilon}\\sim p_\\varepsilon}\\left[D_m(G_m(\\boldsymbol{\\varepsilon}))\\right],\n",
    "  \\\\\n",
    "  \\mathcal{L}_x(D_x, G_x, G_m) &=\n",
    "  \\mathbb{E}_{(\\mathbf{x},\\mathbf{m})\\sim p_\\mathcal{D}}\\left[D_x(f_\\tau(\\mathbf{x},\\mathbf{m}))\\right] -\n",
    "  \\mathbb{E}_{\\boldsymbol{\\varepsilon}\\sim p_\\varepsilon, \\mathbf{z}\\sim p_z}\\left[\n",
    "  D_x\\left(f_\\tau\\left(G_x(\\mathbf{z}),G_m(\\boldsymbol{\\varepsilon})\\right)\\right)\\right].\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We optimize the generators and the discriminators\n",
    "according to the following objectives:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "  &\\min_{G_x}\\max_{D_x\\in\\mathcal{F}_x} \\mathcal{L}_x(D_x, G_x, G_m),\n",
    "  \\\\\n",
    "  &\\min_{G_m}\\max_{D_m\\in\\mathcal{F}_m} \\mathcal{L}_m(D_m, G_m) + \\alpha \\mathcal{L}_x(D_x, G_x, G_m),\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where $\\mathcal{F}_x, \\mathcal{F}_m$ are defined such that $D_x, D_m$ are\n",
    "both 1-Lipschitz in Wasserstein GANs.\n",
    "We alternate between `n_critic` steps of optimizing the discriminators\n",
    "and one step of optimizing the generators.\n",
    "The update of the discriminator is implemented in `CriticUpdater` described\n",
    "earlier, which handles the Lipschitz constraint.\n",
    "We use $\\alpha=0.2$ to encourage the generated masks to match the distribution\n",
    "of the real masks and the masked generated complete samples\n",
    "to match masked real data.\n",
    "\n",
    "In the example, \n",
    "we use the standard Gaussian $\\mathcal{N}(\\mathbf{0}, \\mathbf{I})$ \n",
    "for both noise distributions $p_z$ and $p_\\varepsilon$.\n",
    "\n",
    "During training, we draw a bunch of generated complete data and mask samples\n",
    "every `plot_interval` epochs to assess the training progress qualitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interval = 50\n",
    "critic_updates = 0\n",
    "\n",
    "for epoch in range(300):\n",
    "    for real_data, real_mask, _ in data_loader:\n",
    "\n",
    "        real_data = real_data.to(device)\n",
    "        real_mask = real_mask.to(device).float()\n",
    "\n",
    "        # Update discriminators' parameters\n",
    "        data_noise.normal_()\n",
    "        mask_noise.normal_()\n",
    "\n",
    "        fake_data = data_gen(data_noise)\n",
    "        fake_mask = mask_gen(mask_noise)\n",
    "\n",
    "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "        masked_real_data = mask_data(real_data, real_mask)\n",
    "\n",
    "        update_data_critic(masked_real_data, masked_fake_data)\n",
    "        update_mask_critic(real_mask, fake_mask)\n",
    "\n",
    "        critic_updates += 1\n",
    "\n",
    "        if critic_updates == n_critic:\n",
    "            critic_updates = 0\n",
    "\n",
    "            # Update generators' parameters\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "            data_gen.zero_grad()\n",
    "            mask_gen.zero_grad()\n",
    "\n",
    "            data_noise.normal_()\n",
    "            mask_noise.normal_()\n",
    "\n",
    "            fake_data = data_gen(data_noise)\n",
    "            fake_mask = mask_gen(mask_noise)\n",
    "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "            data_loss = -data_critic(masked_fake_data).mean()\n",
    "            data_loss.backward(retain_graph=True)\n",
    "            data_gen_optimizer.step()\n",
    "\n",
    "            data_loss_d = data_loss.detach()\n",
    "            mask_loss = -mask_critic(fake_mask).mean()\n",
    "            (mask_loss + data_loss_d * alpha).backward()\n",
    "            mask_gen_optimizer.step()\n",
    "\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    if plot_interval > 0 and (epoch + 1) % plot_interval == 0:\n",
    "        # Although it makes no difference setting eval() in this example, \n",
    "        # you will need those if you are going to use modules such as \n",
    "        # batch normalization or dropout in the generators.\n",
    "        data_gen.eval()\n",
    "        mask_gen.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            print('Epoch:', epoch)\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 2.5))\n",
    "            \n",
    "            data_noise.normal_()\n",
    "            data_samples = data_gen(data_noise)\n",
    "            plot_grid(ax1, data_samples, title='generated complete data')\n",
    "            \n",
    "            mask_noise.normal_()\n",
    "            mask_samples = mask_gen(mask_noise)\n",
    "            plot_grid(ax2, mask_samples, title='generated masks')\n",
    "            \n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "        data_gen.train()\n",
    "        mask_gen.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing data imputation\n",
    "\n",
    "Now, we introduce an extension of MisGAN for missing data imputation.\n",
    "The goal of missing data imputation is to complete the\n",
    "missing data according to $p(\\mathbf{x}_\\text{mis}|\\mathbf{x}_\\text{obs})$.\n",
    "To do so, we augment MisGAN with an imputer $G_i$ accompanied by\n",
    "a corresponding discriminator $D_i$ as illustrated below.\n",
    "\n",
    "<img src=\"img/misgan-impute.png\" width=\"625\" style=\"display: block; margin: 2em auto\" />\n",
    "\n",
    "The imputer $G_m$ is a function of the incomplete example\n",
    "$(\\mathbf{x},\\mathbf{m})$ and a random vector $\\boldsymbol{\\omega}$\n",
    "drawn from a noise distribution $p_\\omega$.\n",
    "The noise $\\boldsymbol{\\omega}$ is for modeling the uncertainty when sampling\n",
    "from $p(\\mathbf{x}_\\text{mis}|\\mathbf{x}_\\text{obs})$.\n",
    "The imputer is trained by making the completed data $\\hat{\\mathbf{x}}$\n",
    "indistinguishable from the generated complete data $\\tilde{\\mathbf{x}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer\n",
    "\n",
    "We construct the imputer $G_i(\\mathbf{x},\\mathbf{m},\\boldsymbol{\\omega})$\n",
    "as the follows:\n",
    "\n",
    "$$\n",
    "G_i(\\mathbf{x},\\mathbf{m},\\boldsymbol{\\omega})\n",
    "=\\mathbf{x}\\odot\\mathbf{m}+\\hat{G}_i(\\mathbf{x}\\odot\\mathbf{m} + \\boldsymbol{\\omega}\\odot\\bar{\\mathbf{m}})\\odot\\bar{\\mathbf{m}}.\n",
    "$$\n",
    "\n",
    "where $\\hat{G}_i$ is a imputer network that generates the imputation result.\n",
    "\n",
    "The masking on the input of the imputer network,\n",
    "$\\mathbf{x}\\odot\\mathbf{m} + \\boldsymbol{\\omega}\\odot\\bar{\\mathbf{m}}$,\n",
    "ensures that the amount of noise injected to $\\hat{G}_i$\n",
    "is complementary to the size of the observed features.\n",
    "This is intuitive in the sense that when a data case is almost fully-observed,\n",
    "we expect less variety in $p(\\mathbf{x}_\\text{mis}|\\mathbf{x}_\\text{obs})$\n",
    "and vice versa.\n",
    "Note that the noise $\\boldsymbol{\\omega}$ needs to have the same dimensionality\n",
    "as $\\mathbf{x}$.\n",
    "\n",
    "The final masking (outside of $\\hat{G}_i$)\n",
    "ensures that the observed entries of $\\mathbf{x}$ are kept\n",
    "intact in the output of the imputer $G_i$.\n",
    "\n",
    "We implement $G_i$ in `Imputer` as a three-layer fully-connected network\n",
    "with ReLUs in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Imputer(nn.Module):\n",
    "    def __init__(self, arch=(512, 512)):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(784, arch[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[0], arch[1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[1], arch[0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(arch[0], 784),\n",
    "        )\n",
    "\n",
    "    def forward(self, data, mask, noise):\n",
    "        net = data * mask + noise * (1 - mask)\n",
    "        net = net.view(data.shape[0], -1)\n",
    "        net = self.fc(net)\n",
    "        net = torch.sigmoid(net).view(data.shape)\n",
    "        return data * mask + net * (1 - mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the imputer and the corresponding discriminator.\n",
    "We also use the Adam optimizer to train them.\n",
    "Note that for MisGAN imputation, we will re-use most of the components\n",
    "created earlier for MisGAN including the data/mask generators and\n",
    "the discriminators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = Imputer().to(device)\n",
    "impu_critic = ConvCritic().to(device)\n",
    "impu_noise = torch.empty(batch_size, 1, 28, 28, device=device)\n",
    "\n",
    "imputer_lrate = 2e-4\n",
    "imputer_optimizer = optim.Adam(\n",
    "    imputer.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
    "impu_critic_optimizer = optim.Adam(\n",
    "    impu_critic.parameters(), lr=imputer_lrate, betas=(.5, .9))\n",
    "update_impu_critic = CriticUpdater(\n",
    "    impu_critic, impu_critic_optimizer, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training MisGAN imputer\n",
    "\n",
    "To train the imputer-equipped MisGAN,\n",
    "we define the following losses (MisGAN uses the first two):\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "  \\mathcal{L}_m(D_m, G_m) &=\n",
    "  \\mathbb{E}_{(\\mathbf{x},\\mathbf{m})\\sim p_\\mathcal{D}}\\left[D_m(\\mathbf{m})\\right] -\n",
    "  \\mathbb{E}_{\\boldsymbol{\\varepsilon}\\sim p_\\varepsilon}\\left[D_m(G_m(\\boldsymbol{\\varepsilon}))\\right],\n",
    "  \\\\\n",
    "  \\mathcal{L}_x(D_x, G_x, G_m) &=\n",
    "  \\mathbb{E}_{(\\mathbf{x},\\mathbf{m})\\sim p_\\mathcal{D}}\\left[D_x(f_\\tau(\\mathbf{x},\\mathbf{m}))\\right] -\n",
    "  \\mathbb{E}_{\\boldsymbol{\\varepsilon}\\sim p_\\varepsilon, \\mathbf{z}\\sim p_z}\\left[\n",
    "  D_x\\left(f_\\tau\\left(G_x(\\mathbf{z}),G_m(\\boldsymbol{\\varepsilon})\\right)\\right)\\right],\n",
    "  \\\\\n",
    "  \\mathcal{L}_i(D_i, G_i, G_x) &=\n",
    "  \\mathbb{E}_{\\mathbf{z}\\sim p_z}\\left[D_i(G_x(\\mathbf{z}))\\right] -\n",
    "  \\mathbb{E}_{(\\mathbf{x},\\mathbf{m})\\sim p_\\mathcal{D}, \\boldsymbol{\\omega}\\sim p_\\omega}\n",
    "  \\left[D_i(G_i(\\mathbf{x},\\mathbf{m},\\boldsymbol{\\omega}))\\right].\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "We jointly learn the data generating process and\n",
    "the imputer according to the following objectives:\n",
    "\n",
    "$$\n",
    "\\begin{split}\n",
    "\\min_{G_i}\\max_{D_i\\in\\mathcal{F}_i}\\ &\\mathcal{L}_i(D_i, G_i, G_x),\n",
    "\\label{eq:objimputer}\\\\\n",
    "\\min_{G_x}\\max_{D_x\\in\\mathcal{F}_x}\\ &\\mathcal{L}_x(D_x, G_x, G_m) + \\beta \\mathcal{L}_i(D_i, G_i, G_x),\n",
    "\\\\\n",
    "\\min_{G_m}\\max_{D_m\\in\\mathcal{F}_m}\\ &\\mathcal{L}_m(D_m, G_m) + \\alpha \\mathcal{L}_x(D_x, G_x, G_m),\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where we use $\\beta=0.1$ to encourage the generated complete data to match\n",
    "the distribution of the imputed real data in addition to\n",
    "having the masked generated data to match the masked real data.\n",
    "\n",
    "In the MNIST example, the noise for the imputer $\\boldsymbol{\\omega}$\n",
    "(`impu_noise`) is drawn from $p_\\omega=\\operatorname{uniform}(0, 1)$\n",
    "to match the range of the pixel value \\[0, 1\\].\n",
    "\n",
    "We plot the imputation results every `plot_interval` epochs.\n",
    "In each of the plots,\n",
    "the region inside of each red box are the observed pixels;\n",
    "the pixels outside of the box are generated by the imputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta = .1\n",
    "plot_interval = 100\n",
    "critic_updates = 0\n",
    "\n",
    "for epoch in range(600):\n",
    "    for real_data, real_mask, index in data_loader:\n",
    "\n",
    "        real_data = real_data.to(device)\n",
    "        real_mask = real_mask.to(device).float()\n",
    "\n",
    "        masked_real_data = mask_data(real_data, real_mask)\n",
    "\n",
    "        # Update discriminators' parameters\n",
    "        data_noise.normal_()\n",
    "        fake_data = data_gen(data_noise)\n",
    "\n",
    "        mask_noise.normal_()\n",
    "        fake_mask = mask_gen(mask_noise)\n",
    "        masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "        impu_noise.uniform_()\n",
    "        imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "\n",
    "        update_data_critic(masked_real_data, masked_fake_data)\n",
    "        update_mask_critic(real_mask, fake_mask)\n",
    "        update_impu_critic(fake_data, imputed_data)\n",
    "\n",
    "        critic_updates += 1\n",
    "\n",
    "        if critic_updates == n_critic:\n",
    "            critic_updates = 0\n",
    "\n",
    "            # Update generators' parameters\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "            for p in impu_critic.parameters():\n",
    "                p.requires_grad_(False)\n",
    "\n",
    "            data_noise.normal_()\n",
    "            fake_data = data_gen(data_noise)\n",
    "\n",
    "            mask_noise.normal_()\n",
    "            fake_mask = mask_gen(mask_noise)\n",
    "            masked_fake_data = mask_data(fake_data, fake_mask)\n",
    "\n",
    "            impu_noise.uniform_()\n",
    "            imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "\n",
    "            data_loss = -data_critic(masked_fake_data).mean()\n",
    "            mask_loss = -mask_critic(fake_mask).mean()\n",
    "            impu_loss = -impu_critic(imputed_data).mean()\n",
    "\n",
    "            mask_gen.zero_grad()\n",
    "            (mask_loss + data_loss * alpha).backward(retain_graph=True)\n",
    "            mask_gen_optimizer.step()\n",
    "\n",
    "            data_gen.zero_grad()\n",
    "            (data_loss + impu_loss * beta).backward(retain_graph=True)\n",
    "            data_gen_optimizer.step()\n",
    "\n",
    "            imputer.zero_grad()\n",
    "            impu_loss.backward()\n",
    "            imputer_optimizer.step()\n",
    "\n",
    "            for p in data_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in mask_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "            for p in impu_critic.parameters():\n",
    "                p.requires_grad_(True)\n",
    "\n",
    "    if plot_interval > 0 and (epoch + 1) % plot_interval == 0:\n",
    "        with torch.no_grad():\n",
    "            imputer.eval()\n",
    "\n",
    "            # Plot imputation results\n",
    "            impu_noise.uniform_()\n",
    "            imputed_data = imputer(real_data, real_mask, impu_noise)\n",
    "            bbox = [data.mask_loc[idx] for idx in index]\n",
    "            print('Epoch:', epoch)\n",
    "            fig, ax = plt.subplots(figsize=(6, 3))\n",
    "            plot_grid(ax, imputed_data, bbox, gap=2)\n",
    "            plt.show()\n",
    "            plt.close(fig)\n",
    "\n",
    "            imputer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.15 ('py36_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "0a26cd162e81aa4005801bb611935820552b6c0887446264c8ff131a85fa1248"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
