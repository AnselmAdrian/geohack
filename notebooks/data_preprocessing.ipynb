{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a9f2c8b-0e85-4cda-a4e1-1380e2adf780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.models import save_model, load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, UpSampling2D\n",
    "from keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Model\n",
    "import time\n",
    "from gmm import custom_autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399ab51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d75f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def _nafy_duplicates(ar, chunk_size = 3):\n",
    "    # If `chunk_size` consecutive values appear before OR after, make into NaN.\n",
    "    or_up = [ar[i:-i] - ar[2*i:] == 0 for i in range(1, chunk_size+1)]\n",
    "    or_down = [ar[i:-i] - ar[2*i:] == 0 for i in range(1, chunk_size+1)]\n",
    "    fix_len_up = [np.concatenate([[True]*i, or_up[i-1], [True]*i]) for i in range(1, chunk_size+1)]\n",
    "    fix_len_down = [np.concatenate([[True]*i, or_down[i-1], [True]*i]) for i in range(1, chunk_size+1)]\n",
    "    all_up = np.all(np.stack(fix_len_up, axis=1), axis=1)\n",
    "    all_down = np.all(np.stack(fix_len_down, axis=1), axis=1)\n",
    "    ar[all_up | all_down] = np.nan\n",
    "\n",
    "def clean_data(las: lasio.LASFile, chunk_size: int = 3, ignore_contains: List = None) -> None:\n",
    "    \"\"\"If `chunk_size` consecutive values are recorded, make into NaN.\n",
    "\n",
    "    Args:\n",
    "        las (lasio.LASFile): The LAS file.\n",
    "        chunk_size (int, optional): Number of consecutive values. Defaults to 3.\n",
    "        ignore_contains (List, optional): If there are words in ABBR to ignore. Defaults to [\"LITHO\"].\n",
    "    \"\"\"\n",
    "    ignore_contains = [\"LITHO\"] if ignore_contains is None else ignore_contains\n",
    "    for key, track in las.curvesdict.items():\n",
    "        if any(s in key for s in ignore_contains):\n",
    "            continue\n",
    "        _nafy_duplicates(track.data, chunk_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "398d1873",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/Non-seismic/03 FORCE/FORCE 2020 Wells'\n",
    "filenames = os.listdir(folder)\n",
    "\n",
    "df_list = []\n",
    "for filename in filenames:\n",
    "    if 'las' in filename:\n",
    "        data = lasio.read(os.path.join(folder, filename))\n",
    "        # Move to clean_data.\n",
    "        clean_data(data)\n",
    "        rename_map = {x.mnemonic: x.descr.split(' ')[1] for x in data.curves}\n",
    "          \n",
    "        df = data.df().copy()\n",
    "        df = df.rename(columns = rename_map)\n",
    "        df['filename'] = filename\n",
    "        df_list.append(df)\n",
    "df_master = pd.concat(df_list, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d62fe44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master.to_csv('/home/geouser05/geo/data/02_preprocessed/well_logs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbfba0bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BS</th>\n",
       "      <th>CALI</th>\n",
       "      <th>DEPTH_MD</th>\n",
       "      <th>DRHO</th>\n",
       "      <th>DTC</th>\n",
       "      <th>DTS</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_CONFIDENCE</th>\n",
       "      <th>FORCE_2020_LITHOFACIES_LITHOLOGY</th>\n",
       "      <th>GR</th>\n",
       "      <th>NPHI</th>\n",
       "      <th>...</th>\n",
       "      <th>z_loc</th>\n",
       "      <th>filename</th>\n",
       "      <th>RSHA</th>\n",
       "      <th>RXO</th>\n",
       "      <th>SP</th>\n",
       "      <th>RMIC</th>\n",
       "      <th>SGR</th>\n",
       "      <th>ROPA</th>\n",
       "      <th>DCAL</th>\n",
       "      <th>MUDWEIGHT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DEPT</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>NaN</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35_11-12_logs.las</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362.4024</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35_11-12_logs.las</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362.5548</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35_11-12_logs.las</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362.7072</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35_11-12_logs.las</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362.8596</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>35_11-12_logs.las</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          BS  CALI  DEPTH_MD  DRHO  DTC  DTS  \\\n",
       "DEPT                                           \n",
       "NaN      NaN   NaN       NaN   NaN  NaN  NaN   \n",
       "362.4024 NaN   NaN       NaN   NaN  NaN  NaN   \n",
       "362.5548 NaN   NaN       NaN   NaN  NaN  NaN   \n",
       "362.7072 NaN   NaN       NaN   NaN  NaN  NaN   \n",
       "362.8596 NaN   NaN       NaN   NaN  NaN  NaN   \n",
       "\n",
       "          FORCE_2020_LITHOFACIES_CONFIDENCE  FORCE_2020_LITHOFACIES_LITHOLOGY  \\\n",
       "DEPT                                                                            \n",
       "NaN                                     NaN                               NaN   \n",
       "362.4024                                NaN                               NaN   \n",
       "362.5548                                NaN                               NaN   \n",
       "362.7072                                NaN                               NaN   \n",
       "362.8596                                NaN                               NaN   \n",
       "\n",
       "          GR  NPHI  ...  z_loc           filename  RSHA  RXO  SP  RMIC  SGR  \\\n",
       "DEPT                ...                                                       \n",
       "NaN      NaN   NaN  ...    NaN  35_11-12_logs.las   NaN  NaN NaN   NaN  NaN   \n",
       "362.4024 NaN   NaN  ...    NaN  35_11-12_logs.las   NaN  NaN NaN   NaN  NaN   \n",
       "362.5548 NaN   NaN  ...    NaN  35_11-12_logs.las   NaN  NaN NaN   NaN  NaN   \n",
       "362.7072 NaN   NaN  ...    NaN  35_11-12_logs.las   NaN  NaN NaN   NaN  NaN   \n",
       "362.8596 NaN   NaN  ...    NaN  35_11-12_logs.las   NaN  NaN NaN   NaN  NaN   \n",
       "\n",
       "          ROPA DCAL  MUDWEIGHT  \n",
       "DEPT                            \n",
       "NaN        NaN  NaN        NaN  \n",
       "362.4024   NaN  NaN        NaN  \n",
       "362.5548   NaN  NaN        NaN  \n",
       "362.7072   NaN  NaN        NaN  \n",
       "362.8596   NaN  NaN        NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_master.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a918198",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = '/mnt/Non-seismic/01 Poseidon/Spliced well logs provided by occam technology'\n",
    "filenames = os.listdir(folder)\n",
    "\n",
    "df_list = []\n",
    "for filename in filenames:\n",
    "    if 'LAS' in filename:\n",
    "        data = lasio.read(os.path.join(folder, filename))\n",
    "        # Move to clean_data.\n",
    "        #clean_data(data)\n",
    "        #rename_map = {x.mnemonic: x.descr.split(' ')[1] for x in data.curves}\n",
    "          \n",
    "        df = data.df().copy()\n",
    "        #df = df.rename(columns = rename_map)\n",
    "        df['filename'] = filename\n",
    "        df['field'] = 'Pesidon'\n",
    "        df_list.append(df)\n",
    "df_master2 = pd.concat(df_list, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe965fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_base = [\n",
    "            # 'CALI', \n",
    "            # 'DRHO', \n",
    "            'DTC', \n",
    "            # 'FORCE_2020_LITHOFACIES_CONFIDENCE', \n",
    "            'FORCE_2020_LITHOFACIES_LITHOLOGY',\n",
    "            'GR', \n",
    "            'NPHI', \n",
    "            # 'PEF', \n",
    "            'RDEP', \n",
    "            'RHOB', \n",
    "            'RMED', \n",
    "            # 'ROP',\n",
    "            # 'RSHA', \n",
    "            # 'RXO', \n",
    "            # 'SP', \n",
    "            # 'RMIC', \n",
    "            # 'SGR', \n",
    "            # 'ROPA', \n",
    "            # 'DCAL',\n",
    "            # 'MUDWEIGHT'\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c4b845d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCE_2020_LITHOFACIES_LITHOLOGY\n",
      "RDEP\n",
      "RMED\n"
     ]
    }
   ],
   "source": [
    "for f in features_base:\n",
    "    if f not in df_master2.columns:\n",
    "        print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ecc1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master2.to_csv('/home/geouser05/geo/data/02_preprocessed/well_logs2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1223d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATRT\n",
      "ATRX\n",
      "BATC\n",
      "CAL1\n",
      "DCAV\n",
      "DTC\n",
      "DTCO\n",
      "DTS\n",
      "DTSM\n",
      "ECGR\n",
      "GR\n",
      "GRARC\n",
      "GRD\n",
      "HDAR\n",
      "HROM\n",
      "HTNP\n",
      "NPHI\n",
      "P16H\n",
      "P34H\n",
      "RD\n",
      "RHOB\n",
      "RHOZ\n",
      "RS\n",
      "TNP\n",
      "TNPH\n",
      "filename\n"
     ]
    }
   ],
   "source": [
    "for x in df_master2.columns.sort_values():\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6a95de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39cd5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9f9caaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PoseidonNorth1Decim.LAS',\n",
       " 'Torosa1Decim.LAS',\n",
       " 'Pharos1Decim.LAS',\n",
       " 'Poseidon2Decim.LAS',\n",
       " 'README FIRST.docx',\n",
       " 'Kronos1Decim.LAS',\n",
       " 'Checkshots',\n",
       " 'Poseidon1Decim.LAS',\n",
       " 'Proteus1Decim.LAS',\n",
       " 'Boreas1Decim.LAS']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder = '/mnt/Non-seismic/01 Poseidon/Spliced well logs provided by occam technology'\n",
    "os.listdir(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a8d0405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS\n",
      "CALI\n",
      "DEPTH_MD\n",
      "DRHO\n",
      "DTC\n",
      "DTS\n",
      "FORCE_2020_LITHOFACIES_CONFIDENCE\n",
      "FORCE_2020_LITHOFACIES_LITHOLOGY\n",
      "GR\n",
      "NPHI\n",
      "PEF\n",
      "RDEP\n",
      "RHOB\n",
      "RMED\n",
      "ROP\n",
      "x_loc\n",
      "y_loc\n",
      "z_loc\n",
      "filename\n",
      "RSHA\n",
      "RXO\n",
      "SP\n",
      "RMIC\n",
      "SGR\n",
      "ROPA\n",
      "DCAL\n",
      "MUDWEIGHT\n"
     ]
    }
   ],
   "source": [
    "for f in df_master.columns:\n",
    "    print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfed4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8092256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Dense(16*16*8*8, use_bias=False, input_shape=(100,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Reshape((32, 32, 16)))\n",
    "    assert model.output_shape == (None,32, 32, 16)  # Note: None is the batch size\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(1, 1), padding='same', activation='selu'))\n",
    "    print(model.output_shape)\n",
    "    assert model.output_shape == (None, 32, 32, 64)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same', activation='selu'))\n",
    "    print(model.output_shape)\n",
    "    assert model.output_shape == (None, 16, 16, 64)\n",
    "    #model.add(layers.BatchNormalization())\n",
    "    #model.add(layers.LeakyReLU())\n",
    "\n",
    "    model.add(layers.Conv2D(8, (5, 5), strides=(1, 1), padding='same', use_bias=False, activation='tanh'))\n",
    "    assert model.output_shape == (None, 16, 16, 8)\n",
    "\n",
    "    return model\n",
    "\n",
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same',\n",
    "                                     input_shape=[16, 16, 8]))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "\n",
    "# Notice the use of `tf.function`\n",
    "# This annotation causes the function to be \"compiled\".\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "      generated_images = generator(noise, training=True)\n",
    "\n",
    "      real_output = discriminator(images, training=True)\n",
    "      fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "      disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d58159b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs, decoder = None):\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      train_step(image_batch)\n",
    "\n",
    "    # Produce images for the GIF as you go\n",
    "    display.clear_output(wait=True)\n",
    "    generate_and_save_images(generator,\n",
    "                             epoch + 1,\n",
    "                             seed,\n",
    "                             decoder = decoder)\n",
    "\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch + 1) % 15 == 0:\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "\n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "\n",
    "  # Generate after the final epoch\n",
    "  display.clear_output(wait=True)\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs,\n",
    "                           seed, decoder = decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3f249c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('py37_tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cbb753d1e2d2fedf73db84e0875b5009aa0b6e6dc266a22f39d7c2ae07436c2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
