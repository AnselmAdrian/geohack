{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import missingno as mn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('/home/geouser05/geo/notebooks/prediction_dts.csv')\n",
    "df = pd.read_csv('E:/Public/GeoHack/data/02_preprocessed/well_logs.csv')\n",
    "#df = pd.read_csv('E:/Public/GeoHack/data/07_model_output/prediction_dts_full2.csv')\n",
    "df = df.iloc[:, 1:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features_base = [\n",
    "            # 'CALI', \n",
    "            # 'DRHO', \n",
    "            'DTC', \n",
    "            # 'FORCE_2020_LITHOFACIES_CONFIDENCE', \n",
    "            #'FORCE_2020_LITHOFACIES_LITHOLOGY',\n",
    "            'GR', \n",
    "            'NPHI', \n",
    "            # 'PEF', \n",
    "            #'RDEP', \n",
    "            'RHOB', \n",
    "            #'RMED', \n",
    "            # 'ROP',\n",
    "            # 'RSHA', \n",
    "            # 'RXO', \n",
    "            # 'SP', \n",
    "            # 'RMIC', \n",
    "            # 'SGR', \n",
    "            # 'ROPA', \n",
    "            # 'DCAL',\n",
    "            # 'MUDWEIGHT'\n",
    "            ]\n",
    "features_one_hot_encode = ['FORCE_2020_LITHOFACIES_LITHOLOGY']\n",
    "features_scale = ['GR', 'NPHI', 'DTC', 'RHOB'] #list(set(features_base) - set(features_one_hot_encode))\n",
    "target = 'DTS'\n",
    "n = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:/Public/GeoHack/conf/parameters.yml', 'r') as file:\n",
    "    parameters = yaml.safe_load(file)\n",
    "raw_filenames = parameters['filename_dts']\n",
    "def process_filename(x):\n",
    "    return x.replace('/', '_').replace(' ', '_') + '_logs.las'\n",
    "filenames = [process_filename(x) for x in raw_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split well for train and test\n",
    "test_idx = [3, 22,  5, 33, 35,  0, 20,  5]\n",
    "train_idx = list(set(range(len(filenames))) - set(test_idx))\n",
    "\n",
    "# Create train and test dataframes\n",
    "def get_filename(df, f_list, idx_list):\n",
    "    f_list = [x for i, x in enumerate(f_list) if i in idx_list]\n",
    "    return df[df.filename.isin(f_list)].copy()\n",
    "\n",
    "df_train = get_filename(df, filenames, train_idx)\n",
    "df_test = get_filename(df, filenames, test_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    #nc = OneHotEncoder(sparse = False)\n",
    "    #enc.fit(df[features_one_hot_encode])\n",
    "\n",
    "    # with open('E:/Public/GeoHack/data/06_model/onehot_encoder.pkl', 'wb') as file:\n",
    "    #     pickle.dump(enc, file)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df_train[[target] + features_scale])\n",
    "\n",
    "    with open('E:/Public/GeoHack/data/06_model/scale.pkl', 'wb') as file:\n",
    "        pickle.dump(scaler, file)\n",
    "else:\n",
    "    with open('/home/geouser05/geo/data/06_model/onehot_encoder.pkl', 'rb') as file:\n",
    "        enc = pickle.load(file)\n",
    "    encoded_columns = list(enc.categories_[0])[:-1]\n",
    "\n",
    "    with open('/home/geouser05/geo/data/06_model/scale.pkl', 'rb') as file:\n",
    "        scaler = pickle.load(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataset\n",
    "df_preprocessed = df.copy()\n",
    "\n",
    "# df_encode = enc.transform(df[features_one_hot_encode])\n",
    "# encoded_columns = list(enc.categories_[0])\n",
    "# df_preprocessed[encoded_columns] = df_encode\n",
    "# encoded_columns = encoded_columns[:-1]\n",
    "\n",
    "df_preprocessed[[target] + features_scale] = scaler.transform(df[[target] + features_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = features_scale #+ encoded_columns\n",
    "n_features = len(features) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filter = df[df_preprocessed .filename.isin(filenames)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn.matrix(df_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slices(df_train, feature, target, filter = True):\n",
    "    X_train = []\n",
    "    index_arr = []\n",
    "    for well_name in df_train.filename.unique():\n",
    "        df_i = df_train[df_train.filename == well_name].copy()\n",
    "        df_i = df_i[[target] + feature]\n",
    "\n",
    "        \n",
    "        x = df_i.values\n",
    "        min_idx = x.shape[0] - (x.shape[0] // n) * n\n",
    "        y = x[min_idx:,:].copy()\n",
    "        \n",
    "        idx_list = df_i.index[min_idx:].copy()\n",
    "        y = y.reshape(-1, n, x.shape[1])\n",
    "\n",
    "        #target_idx = list(df_i.columns).index(target)\n",
    "        if filter:\n",
    "            target_idx = 0\n",
    "            mask = y[:,:, target_idx]\n",
    "            mask = np.isnan(mask.astype('float64'))\n",
    "            mask = mask.sum(axis = 1) < 0.9 * n\n",
    "            n0 = y.shape[0]\n",
    "            y = y[mask,:,:]\n",
    "            n1 = y.shape[0]\n",
    "            print(f'{well_name:20}: {n0:4} to {n1:4} segments')\n",
    "        X_train.append(y.copy())\n",
    "        index_arr.append(idx_list)\n",
    "\n",
    "    X_train = np.concatenate(X_train, axis = 0)\n",
    "    index_arr = np.concatenate(index_arr, axis = 0)\n",
    "    \n",
    "    return X_train, index_arr, [target] + feature\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_train[encoded_columns] = enc.transform(df_train[features_one_hot_encode])[:, :-1]\n",
    "df_train[[target] + features_scale] = scaler.transform(df_train[[target] + features_scale])\n",
    "\n",
    "df_full = df.copy()\n",
    "#df_full[encoded_columns] = enc.transform(df_full[features_one_hot_encode])[:, :-1]\n",
    "df_full[[target] + features_scale] = scaler.transform(df_full[[target] + features_scale])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _, _ = generate_slices(df_train, feature = features, target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full, index_arr, columns_names = generate_slices(df_full, feature = features, target = target, filter = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_full.shape, index_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 1, n * n_features)\n",
    "X_full = X_full.reshape(-1, 1, n * n_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_builder(num_fet = 80):\n",
    "      # 80 * 2 = 162\n",
    "  generator = keras.models.Sequential([\n",
    "    keras.layers.Input(shape = num_fet * 2), \n",
    "    keras.layers.Dense(32, 'relu'),\n",
    "    keras.layers.Dense(16, 'relu'),\n",
    "    keras.layers.Dense(16, 'relu'),\n",
    "    keras.layers.Dense(16, 'relu'),\n",
    "    keras.layers.Dense(num_fet, 'linear')\n",
    "  ])\n",
    "  return generator\n",
    "\n",
    "def discriminator_builder(num_fet = 80):\n",
    "      # 80 * 2 = 162\n",
    "  discriminator = keras.models.Sequential([\n",
    "      keras.layers.Input(shape = num_fet * 2),\n",
    "      keras.layers.Dense(32, 'relu'),\n",
    "      keras.layers.Dense(16, 'relu'),\n",
    "      keras.layers.Dense(16, 'relu'),\n",
    "      keras.layers.Dense(num_fet, 'sigmoid'),\n",
    "  ])\n",
    "\n",
    "  return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(m, m_hat):\n",
    "    temp = tf.math.multiply(m, tf.math.log(m_hat + 1e-8))\n",
    "    temp += tf.math.multiply(tf.ones_like(m) - m , tf.math.log(tf.ones_like(m_hat) - m_hat + 1e-8))\n",
    "    return -tf.reduce_mean(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(m, m_hat, x, x_hat, alpha = 10):\n",
    "    loss = -tf.math.multiply((tf.ones_like(m)-m), tf.math.log(m_hat+1e-8))\n",
    "    loss += alpha * tf.multiply(m, (x - x_hat) ** 2)\n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hint_smapler(batch_size = 128, num_fet = 80, hint_rate = 0.9):\n",
    "    A = np.random.uniform(0., 1., size = [batch_size, num_fet])\n",
    "    B = hint_rate > A\n",
    "    C = 1.*B\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change Optimizer and learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = keras.optimizers.Adam(1e-6)\n",
    "discriminator_optimizer = keras.optimizers.Adam(1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = generator_builder(num_fet = n_features * n)\n",
    "discriminator = discriminator_builder(num_fet = n_features * n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch, epoch_num):\n",
    "    \n",
    "    m = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), tf.ones_like(batch))\n",
    "    batch_C = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), batch)\n",
    "    z = np.random.uniform(0, 0.01, size = batch.shape)\n",
    "    batch = tf.math.multiply(batch_C, m) + tf.math.multiply((tf.ones_like(m) - m), z)\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "\n",
    "        X_temp = tf.cast(generator(tf.concat([batch, m], 1), training = True), tf.float64)\n",
    "        X_hat = tf.math.multiply(m, batch) + tf.math.multiply((tf.ones_like(m) - m), X_temp)\n",
    "\n",
    "        H = tf.math.multiply(hint_smapler(1, n * n_features),  m)\n",
    "        M_hat = tf.cast(discriminator(tf.concat([X_hat, H], 1), training = True), tf.float64)\n",
    "\n",
    "        disc_loss = discriminator_loss(m, M_hat)\n",
    "        gen_loss = generator_loss(m, M_hat, batch, X_hat, alpha = 10)\n",
    "\n",
    "    gen_grad = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    disc_grad = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "    generator_optimizer.apply_gradients(zip(gen_grad, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients((zip(disc_grad, discriminator.trainable_variables)))\n",
    "    return gen_loss, disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, EPOCHS, model_filepath = None, n_epochs = 10):\n",
    "    losses_gen = []\n",
    "    losses_disc  = []\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        try:\n",
    "            print(f'Epoch {epoch} / {EPOCHS}:', end=' ')\n",
    "            t0 = time.time()\n",
    "            for batch in dataset:\n",
    "                gen_loss, disc_loss = train_step(batch, epoch)\n",
    "                losses_gen.append(gen_loss)\n",
    "                losses_disc.append(disc_loss)\n",
    "            t1 = time.time()\n",
    "            print(f'gen_loss = {gen_loss}, disc_loss = {disc_loss}, time = {t1 - t0}')\n",
    "            if (model_filepath is not None) and (epoch % n_epochs) == 0:\n",
    "                generator.save(model_filepath + f'gen_{epoch}.h5')\n",
    "                discriminator.save(model_filepath + f'disc_{epoch}.h5')\n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    if (model_filepath is not None):\n",
    "        generator.save(model_filepath + f'gen_{epoch}.h5')\n",
    "        discriminator.save(model_filepath + f'disc_{epoch}.h5')\n",
    "    return losses_gen, losses_disc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices(X_train).shuffle(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_loss, disc_loss = train(train_data, EPOCHS = 500) #, model_filepath = '/home/geouser05/geo/data/06_model/', n_epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(gen_loss, color = 'orange')\n",
    "plt.title('Generator Loss')\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (20, 10))\n",
    "plt.plot(disc_loss, color = 'orange')\n",
    "plt.title('Disciriminator Loss')\n",
    "plt.xlabel('Iter')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data = []\n",
    "for batch in X_full:\n",
    "    m = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), tf.ones_like(batch))\n",
    "    batch_C = tf.where(tf.math.is_nan(batch), tf.zeros_like(batch), batch)\n",
    "    z = np.random.uniform(0, 0.01, size = batch.shape)\n",
    "    batch = tf.math.multiply(batch_C, m) + tf.math.multiply((tf.ones_like(m) - m), z)\n",
    "    batch_out = generator(tf.concat([batch, m], 1), training = False)\n",
    "    generated_data.append(batch_out.numpy().copy())\n",
    "generated_data = np.concatenate(generated_data, axis = 0).reshape(-1, n_features)\n",
    "#generated_data = scaler.inverse_transform(generated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_generated = pd.DataFrame(generated_data, columns = [str(x) + '_PRED_GAIN' for x in columns_names], index = index_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out = pd.merge(df, df_generated, left_index = True, right_index = True, how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = df_out['DTS'].isna()\n",
    "df_out['DTS_PRED_GAIN_MASK'] = np.nan\n",
    "df_out['DTS_PRED_GAIN_MASK'][mask] = df_out['DTS_PRED_GAIN'][mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.filename.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i = df_out[df_out.filename == '35_9-10_S_logs.las']\n",
    "px.scatter(df_i, x = ['DTS', 'DTS_PRED_GAIN_MASK'], y = 'DEPTH_MD', animation_frame = 'filename')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_i.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_out.to_csv('/home/geouser05/geo/data/07_model_output/prediction_dts_full2_gain.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_gpu",
   "language": "python",
   "name": "tensorflow_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cbb753d1e2d2fedf73db84e0875b5009aa0b6e6dc266a22f39d7c2ae07436c2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
